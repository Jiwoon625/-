# 서울시 전세 보증금 예측 모델 및 웹 애플리케이션

## 1. 프로젝트 개요

본 프로젝트는 서울시 전월세 실거래가 데이터를 활용하여 **전세 보증금을 예측하는 머신러닝 모델을 개발**하고, 이를 사용자가 쉽게 이용할 수 있는 **웹 애플리케이션으로 배포**하는 것을 목표로 합니다.

200만 건이 넘는 대용량 데이터를 처리하고, 1GB에 달하는 모델을 웹 환경에 배포하는 과정에서 발생한 다양한 기술적 문제들을 해결한 과정을 기록합니다.

## 2. 모델링 과정 및 성능

### 가. 데이터 전처리
- **원본 데이터**: `서울 10년치 전월세계약 전처리 최후수정본.csv`
- **데이터 분리**: 전세와 월세 데이터를 별도의 파일(`서울 10년치 전월세계약_전세.csv`)로 분리하여 모델링에 사용했습니다.
- **특징 생성(Feature Engineering)**:
    - `건물연령`: `접수년도` - `건축년도`
    - `평당_보증금`: `보증금(만원)` / (`임대면적`/3.3)
    - `지하철역까지 직선거리`: `서울시 역사마스터 정보.csv`를 기반으로 각 매물의 좌표(lat, lon)와 가장 가까운 지하철역까지의 직선 거리를 계산하여 추가했습니다.
- **이상치 및 불필요 데이터 제거**: 분석 목적에 맞지 않는 상/하위 1% 데이터, 음수 값 등을 제거하여 데이터 품질을 높였습니다.

### 나. 모델 학습
Orange3와 같은 GUI 툴은 200만 건 이상의 대용량 데이터를 처리하기 어려워, Python(Jupyter Notebook) 환경에서 전체 데이터를 사용해 모델링을 진행했습니다.

1.  **선형 회귀 (SGDRegressor)**
    - **문제점**: 대용량 데이터에 One-Hot Encoding을 적용 시 `MemoryError` 발생.
    - **해결책**: 희소 행렬(Sparse Matrix)을 생성하고, 희소 행렬 처리에 효율적인 `SGDRegressor`를 사용하여 메모리 문제를 해결했습니다.
    - **성능**: 테스트 데이터 기준 **R²=0.7463**, MAE(평균절대오차) 약 9,172만 원의 준수한 성능을 보였습니다.

2.  **그래디언트 부스팅 (XGBoost, 최종 채택 모델)**
    - 선형 모델의 한계를 넘어 비선형 관계와 특징 간의 복잡한 상호작용을 포착하기 위해 Gradient Boosting 모델을 도입했습니다.
    - **데이터 분할**: 시계열 특성을 반영하기 위해 **2023년까지의 데이터를 학습**에, **2024년 이후 데이터를 테스트**에 사용하는 시계열 분할(Time-Split)을 적용했습니다.
    - **성능**:
        - **테스트 데이터 기준 R²(설명력) 0.9266**
        - **MAE(평균절대오차): 약 4,424만 원**
        - 선형 모델 대비 설명력(R² 0.75 → 0.93)과 오차(MAE 9,172만 → 4,424만)가 크게 개선되었습니다.
    - **주요 예측 변수(Feature Importance)**: `건물용도`, `임대면적`, `동_norm`, `구_norm`, `건축년도` 순으로 전세 보증금에 큰 영향을 미치는 것으로 나타났습니다.

## 3. 웹 애플리케이션 배포 과정

1.08GB에 달하는 대용량 모델(`jeonse_gbm_xgb.json`)을 웹에 배포하기 위해 여러 기술적 문제를 해결했습니다.

### 가. 문제점
- **Netlify 배포 실패**: 정적 호스팅 서비스인 Netlify는 대용량 파일 업로드를 지원하지 않아 배포에 실패했습니다.
- **브라우저 메모리 한계**: 설령 Git LFS 등을 통해 파일을 올리더라도, 사용자의 브라우저가 1GB가 넘는 모델을 직접 다운로드하고 실행하는 것은 메모리 초과로 인한 브라우저 다운(`STATUS_BREAKPOINT`)을 유발하여 비현실적이었습니다.

### 나. 해결 과정
1.  **실행 환경 전환 (Client-side → Server-side)**
    - 브라우저의 한계를 극복하기 위해, **서버에서 모델을 실행하는 Streamlit으로 전환**했습니다. 이 방식은 사용자의 기기 사양과 무관하게 안정적인 서비스 제공이 가능합니다.

2.  **모델 파일 최적화 및 호스팅**
    - **포맷 변환**: 텍스트 기반의 JSON 모델을 더 작고 로드 속도가 빠른 바이너리 포맷 **UBJ**(`Universal Binary JSON`)로 변환했습니다.
    - **압축**: 전송량을 더 줄이기 위해 UBJ 파일을 **Gzip**(`.gz`)으로 추가 압축했습니다.
    - **호스팅**: Git LFS의 트래픽 한계를 우회하기 위해, 저장소와는 별도로 대용량 파일을 배포할 수 있는 **GitHub Releases**에 압축된 모델 파일(`jeonse_gbm_xgb.ubj.gz`)을 업로드했습니다.

3.  **대용량 파일 업로드**
    - 179MB로 최적화된 모델 파일이 GitHub 웹의 업로드 제한(25MB)을 초과하여, **GitHub CLI**(Command Line Interface)를 사용해 터미널 명령어로 성공적으로 업로드했습니다.

4.  **애플리케이션 코드 수정**
    - `app.py`: Streamlit 앱 실행 시, GitHub Releases에 있는 모델 URL로부터 압축 파일을 다운로드하고, 압축을 해제하여 메모리에 로드하도록 코드를 수정했습니다. 모델 로딩처럼 무거운 작업은 `@st.cache_resource`로 캐싱하여 성능을 최적화했습니다.
    - `requirements.txt`: 모델 다운로드를 위한 `requests` 라이브러리를 추가했습니다.

최종적으로 이러한 과정을 통해 대용량 모델 기반의 예측 서비스를 **Streamlit Community Cloud**에 성공적으로 배포할 수 있었습니다.

## 4. 사용된 주요 기술
- **데이터 분석 및 모델링**: Pandas, Scikit-learn, XGBoost, LightGBM
- **웹 애플리케이션**: Streamlit
- **배포 및 버전 관리**: Git, GitHub, Git LFS, GitHub CLI, GitHub Releases, Streamlit Community Cloud
- **지도 및 지오코딩**: Folium, Nominatim (OpenStreetMap)

---
```
